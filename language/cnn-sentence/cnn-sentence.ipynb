{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sentence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 47431\n",
      "Num reviews: 64720\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "\n",
    "MOVIE_REVIEW_DIR = pathlib.Path('data/movie_reviews/txt_sentoken/')\n",
    "\n",
    "\n",
    "def load_reviews(path: pathlib.Path) -> list[str]:\n",
    "    \"\"\"Build movie review training set from the dataset directory\"\"\"\n",
    "    x = []\n",
    "    for filename in path.glob(\"*.txt\"):\n",
    "        with filename.open() as f:\n",
    "            x.extend(f.readlines())\n",
    "    return x\n",
    "\n",
    "\n",
    "def tokenize(sentence: str) -> list[str]:\n",
    "    \"\"\"Tokenize the sentence into words.\"\"\"\n",
    "    return sentence.lower().split(' ')\n",
    "\n",
    "\n",
    "MAX_SENTENCE_LEN = 30  # Assumes only first N elements are allowed\n",
    "\n",
    "\n",
    "def build_vocab(contents: list[str]) -> dict[str, int]:\n",
    "    \"\"\"Build an index of words to token number.\"\"\"\n",
    "    vocab: dict[str, int] = {}\n",
    "    vocab['</s>'] = 0\n",
    "    index = 1\n",
    "    for line in load_reviews(MOVIE_REVIEW_DIR / \"pos\") + load_reviews(MOVIE_REVIEW_DIR / \"neg\"):\n",
    "        tokens = tokenize(line)\n",
    "        for word in itertools.islice(tokens, MAX_SENTENCE_LEN):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(load_reviews(MOVIE_REVIEW_DIR / \"pos\") + load_reviews(MOVIE_REVIEW_DIR / \"neg\"))\n",
    "vocab_inv = { v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "def encode_sentence(sentence: str) -> torch.tensor:\n",
    "    \"\"\"Encode a sentence as a series of token indexes.\"\"\"\n",
    "    encoded = [ vocab[word.lower()] for word in itertools.islice(tokenize(sentence), MAX_SENTENCE_LEN) ]\n",
    "    if len(encoded) < MAX_SENTENCE_LEN:\n",
    "        encoded.extend([0] * (MAX_SENTENCE_LEN - len(encoded)))\n",
    "    return torch.tensor(encoded)\n",
    "\n",
    "\n",
    "reviews: list[tuple[str, float]] = []\n",
    "for review in load_reviews(MOVIE_REVIEW_DIR / \"pos\"):\n",
    "    reviews.append((encode_sentence(review), 1.0))\n",
    "for review in load_reviews(MOVIE_REVIEW_DIR / \"neg\"):\n",
    "    reviews.append((encode_sentence(review), 0.0))\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Num reviews: {len(reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000000/3000000 [00:07<00:00, 379677.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47431, 47431)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "WORD2VEC = 'data/GoogleNews-vectors-negative300.bin'\n",
    "MAX_WORDLEN = 50\n",
    "WEIGHT_SIZE = 4\n",
    "WORD_SIZE = 300\n",
    "\n",
    "\n",
    "def load_word2vec(vocab: dict[str, int]) -> torch.tensor:\n",
    "    vocab_vec = numpy.zeros((len(vocab), WORD_SIZE))\n",
    "    #vocab_vec[0] = torch.rand(WORD_SIZE) - 0.5\n",
    "    with open(WORD2VEC, 'rb') as fd:\n",
    "        line = fd.readline()\n",
    "        parts = line.decode('utf-8').split(' ')\n",
    "        words = int(parts[0])\n",
    "        word_size = int(parts[1])\n",
    "        if word_size != WORD_SIZE:\n",
    "            raise ValueError(f\"Unexpected word size {word_size} != {WORD_SIZE}\")\n",
    "        for i in tqdm(range(0, words)):\n",
    "            # Read the next word\n",
    "            s = b''\n",
    "            while True:\n",
    "                ch = fd.read(1)\n",
    "                if ch == b' ':\n",
    "                    break\n",
    "                if ch == b'':\n",
    "                    raise ValueError(\"Unexpected eof\")\n",
    "                if ch != b'\\n':\n",
    "                    s += ch\n",
    "                if len(s) > word_size:\n",
    "                    raise ValueError(f\"Word was too long {s}\")\n",
    "            weights = fd.read(word_size * WEIGHT_SIZE)\n",
    "            wd = numpy.frombuffer(weights, dtype=numpy.float32)\n",
    "            word = s.decode('utf-8').strip().lower()\n",
    "            # Only load words in the vocabulary\n",
    "            if (idx := vocab.get(word)) is not None:\n",
    "                vocab_vec[idx] = wd\n",
    "    return torch.tensor(vocab_vec, dtype=float)\n",
    "\n",
    "\n",
    "vocab2vec = load_word2vec(vocab)\n",
    "len(vocab), len(vocab2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "tensor(-0.2305, dtype=torch.float64)\n",
      "tensor(0.2539, dtype=torch.float64)\n",
      "Sentence example:\n",
      "['my', 'car', 'is', 'a', 'bus']\n",
      "tensor([ 297,  561,    7,   40, 6821,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "def word2vec(word: str) -> torch.Tensor:\n",
    "    \"\"\"Lookup the word using the word2vec encoding or init to random.\"\"\"\n",
    "    if word not in vocab:\n",
    "        word = '</s>'\n",
    "    idx = vocab[word]\n",
    "    return vocab2vec[idx]\n",
    "\n",
    "\n",
    "# Sample from word2vec\n",
    "print(\"Example:\")\n",
    "print(min(word2vec('car')))\n",
    "print(max(word2vec('car')))\n",
    "\n",
    "print(\"Sentence example:\")\n",
    "print(tokenize(\"My car is a bus\"))\n",
    "print(encode_sentence(\"My car is a bus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([51776, 30]), torch.Size([51776]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# From the paper: The input is a concatenated sentence (length n) where each word is\n",
    "# represented as a k-dimensional word vector. X[i, i+j] = concat(Xi, Xi+1, ..., Xi+j)\n",
    "# The sentence is padded where necessary.\n",
    "\n",
    "\n",
    "def build_dataset(dataset: list[tuple[str, float]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    x, y = [], []\n",
    "    for (data, category) in dataset:\n",
    "        x.append(data)\n",
    "        y.append(category)\n",
    "    X = torch.stack(x)\n",
    "    Y = torch.tensor(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.seed(31337)\n",
    "random.shuffle(reviews)\n",
    "n1 = int(0.8*len(reviews))\n",
    "n2 = int(0.9*len(reviews))\n",
    "\n",
    "Xtr, Ytr = build_dataset(reviews[:n1])\n",
    "Xdev, Ydev = build_dataset(reviews[n1:n2])\n",
    "Xte, Yte = build_dataset(reviews[n2:])\n",
    "\n",
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([30]), torch.Size([30]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = reviews[5][0]\n",
    "r2 = reviews[5][0]\n",
    "print((r1.shape, r2.shape))\n",
    "torch.stack([r1, r2]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "FILTER_WINDOWS = (3, 4, 5)\n",
    "FEATURE_MAPS = 100\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"A model for training a CNN text classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings: torch.tensor):\n",
    "        \"\"\"Initialize Model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=True)\n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Conv1D(\n",
    "                in_channels=embeddings.shape,\n",
    "                out_channels=FEATURE_MAPS,\n",
    "                kernel_size=filter_size,\n",
    "            )\n",
    "            for filter_size in FILTER_WINDOWS\n",
    "        ])\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.layers.parameters()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36087, 50847,  1494, 38461, 23165,  9767, 34169, 41167, 38412, 36431,\n",
      "        13991, 11532, 10202, 22085,  8680, 49684, 16303,  5096, 50920, 48551,\n",
      "        17435, 37149, 28246, 34080, 34941,  5518, 18130, 43799, 41903,  4658,\n",
      "        37791, 13978, 11822, 39560, 41644, 24770, 35510, 12431, 12972, 21601,\n",
      "        35592, 24727,  5633, 30474, 23279, 26757, 30857, 49843, 45212, 12928])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 300])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MINI_BATCH_SZ = 50\n",
    "\n",
    "g = torch.Generator().manual_seed(31337)\n",
    "\n",
    "\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (MINI_BATCH_SZ,), generator=g) # (MINI_BATCH_SZ)\n",
    "print(ix)\n",
    "Xb = Xtr[ix]\n",
    "Yb = Ytr[ix]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.shape, Ytr.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
